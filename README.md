# automigrate
Automate the migration of NON-CDB Oracle databases versions 10 through 12 into PDB

OVERVIEW
--------
This project attempts to unify the various methods available for migrating Oracle databases into the Multitenant architecture. 
Database migrations are generally complex projects often involving dozens of steps executed on both source and target systems. Selecting an optimal migration method depends on a number of factors, including version and size of the database being migrated, its availability during the migration as well as any cross platform requirements and network capacity constraints.

This project aims to reduce a cross-platform migration to a minimum of interventions, involving a preparation script on the source database and an execution script on the target database. Only where business needs demand minimum downtime would a third intervention be required on the source database in order to finalize the process.

BACKGROUND
----------
Starting with Oracle v20, the NON-CDB architecture is no longer supported. However, most production Oracle landscapes include at least some v12, v11 and even v10 databases. With increasing emphasis on reducing the enterprise cost of IT infrastructure, there is a growing need to simplify the process of database migration. This project was motivated by the need to migrate some 40 Production Oracle databases, versions 10 through 12 running on AIX to v19 PDBs running on Linux RHEL (nb. v19 is the terminal Oracle database software release offering the longest extended support period). 

The announcement (November, 2019) that 3 PDBs may now run license-free per v19 CDB is further motivation to deliver a more automated migration process.

TECHNICAL DESCRIPTION:
----------------------
With the cross platform transportable tablespace feature becoming available starting with v10, Oracle database migration has become a simpler process, if only in the sense that migration choices became more limited. The advent of Datapump in v9 to replace the antiquated exp/imp utilities has further simplified the available toolset, particularly with the network link option.

What this means is that unless you want to change the database characterset, you can now migrate by moving physical tablespace datafiles instead of logically exporting/importing table rows between source and target. However, this addresses only the need to migrate data segments (tables, indexes, clusters); it does not include metadata like PLSQL packages, views, sequences and other schema objects.

Starting with v 11.2.0.3 it is possible to migrate both tablespaces and metadata in a single datapump operation. This is called Transportable Database and is a tremendous productivity boon. Prior to v 11.2.0.3 there is Transportable Tablespace which covers the migration of data segments but does not cater for metadata. Both migration methods start by placing source application tablespaces into read only mode. Often the biggest constraint in these migrations is the time spent transferring the datafiles to the soure database server. Particularly where network capacity is limited, transporting terabytes of data can take days to complete during which time applications hosted on the source database remain read only and unavailable.

To resolve this problem, the data can be transported by a process of restoring source tablespace datafiles on the target and continuously applying incremental backups; during this "recovery" period the source tablespaces remain fully online and available. Depending on how frequently incremental backups are taken and how much data is generated by the source applications, a final backup will be a relatively small amount of data, taking a proportionally small amount of time to be rolled forward into the target database. To ensure consistency between source and target, the final backup needs to be taken when the application tablespaces are set to read only - this would be when downtime starts.

In this way, all migrations from versions 10, 11 and 12 are performed using the same transportable tablespace feature. Where downtime must be minimized, we have the option to transfer datafiles as a process rather than a one-off operation; whilst this unavoidably complicates the migration, it represents only a small addition to the code base, since it only effects the data transfer process. The remaining integration process is identical for all migration methods.
